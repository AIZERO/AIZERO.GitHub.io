# 批标准化    

`批标准化（batch normalization， BN）`是为了克服神经网络层数加深导致难以训练而诞生的。 我们知道，深度神经网络随着网络深度加深，训练起来会越来越困难，收敛速度会很慢，常常会导致`梯度弥散问题`（vanishing gradient problem）。    

### 目的：

解决由于训练数据与目标数据分布不一致时，训练的模型得不到很好的泛化能力问题。

在训练深层网络时，随着网络的不断加深，各层间输出分布和输入分布将会变得愈加不同。差异不断增大，但是每一处做指向的样本标记（Label）仍是不变的。

### 具体方法：

根据训练样本和目标样本的比例对训练样本做一个矫正。因此，通过引入`批标准化`来规范化某些层或者所有层的输入，从而固定每层输入信号的均值与方差。

批标准化一般用在非线性映射（激活函数）之前，对 $$x=W \cdot u+b $$做规范化，使结果（输出 信号各个维度）的均值为 0，方差为 1。让每一层的输入有一个稳定的分布会有利于网络的训练。    

### 批归一化的优点

- 加大探索的步长，加快收敛的速度；    
- 更容易跳出局部最小值；    